{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4651c468",
   "metadata": {},
   "source": [
    "# Entrenamiento de modelos de aprendizaje con el dataset Polyvore\n",
    "\n",
    "**Autor:** Roberto Araya Day\n",
    "\n",
    "**Fecha de inicio:** 16/11/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9e3d4",
   "metadata": {},
   "source": [
    "## 1. Preparación de datos\n",
    "\n",
    "- El archivo *type_aware_polyvore_outfits.py* es el script de **Learning Type Aware Embeddings for Fashion Compatibility** para cargar el dataset de Polyvore y crear ejemplos. Se define un **TripletImageLoader** con una serie de transformaciones para las imágenes. Este archivo retorna *Triplets* de imágenes postivas y negativas. Modificamos este archivo en la forma de **DoubletImageLoader** para retornar pares de imágenes. Revisar el archivo. \n",
    "- El archivo *similarity_conditions_triplet_image_loader.py* es el script de **Learning similarity Conditions** para cargar triplets de datos de Polyvore. Revisar.\n",
    "\n",
    "- Se dividen en los archivos:\n",
    "    - ***fashion_compatibility_prediction.txt***: En ***fashion_compatibility_prediction_val.txt*** y ***fashion_compatibility_prediction_test.txt*** para las métricas de validación y testeo. Puesto que el archivo contiene **7076** columnas, se ponen **3538** ejemplos en cada archivo.\n",
    "    - ***fill_in_blank_test.txt***.\n",
    "---\n",
    "\n",
    "Los modelos de recomendación están conformados por la siguiente estructura:\n",
    "1. **DoubletImageLoader**: Carga las imágenes, con sus metadatos, y se crean los ejemplos de pares de prendas positivos. Es decir, prendas compatibles. Esta basado en los dataloader de ambas investigaciones señaladas anteriormente.\n",
    "    - Un DataLoader es una estructura que combina un dataset y un sampler, y provee un iterador sobre el dataset.\n",
    "\n",
    "\n",
    "2. **DoubletNet**: Genera los embeddings y calcula la pérdida y distancia entre los vectores obtenidos de los ejemplos positivos. No se necesita para este caso, puesto que el **Modelo General** lo hace por si solo. No necesita obtener el loss o accuracy de los pares de imágenes.\n",
    "\n",
    "\n",
    "3. **Modelo General**: Variante del modelo BYOL que recibe dos imágenes de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455af9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/raraya/miniconda3/envs/fashion_model/lib/python3.10/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce651484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to important folders\n",
    "polyvore_dataset = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'fashion_proj', 'polyvore_data')\n",
    "polyvore_images = os.path.join(polyvore_dataset, 'images')\n",
    "polyvore_info = os.path.join(polyvore_dataset, 'polyvore-info')\n",
    "\n",
    "polyvore_train = os.path.join(polyvore_info, 'train_no_dup')\n",
    "polyvore_val = os.path.join(polyvore_info, 'valid_no_dup')\n",
    "polyvore_test = os.path.join(polyvore_info, 'test_no_dup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23b12075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "import itertools\n",
    "\n",
    "data = {('bags', 'shoes'): 0, ('bags', 'jewellery'): 1, ('bags', 'tops'): 2, ('bags', 'bottoms'): 3,\n",
    "    ('shoes', 'jewellery'): 4, ('shoes', 'tops'): 5, ('shoes', 'bottoms'): 6,\n",
    "    ('jewellery', 'tops'): 7, ('jewellery', 'bottoms'): 8, ('tops', 'bottoms'): 9,\n",
    "    ('bags', 'sunglasses'): 10, ('bags', 'all-body'): 11, ('shoes', 'sunglasses'): 12,\n",
    "    ('shoes', 'all-body'): 13, ('jewellery', 'sunglasses'): 14, ('jewellery', 'all-body'): 15,\n",
    "    ('sunglasses', 'all-body'): 16, ('shoes', 'accessories'): 17, ('jewellery', 'accessories'): 18,\n",
    "    ('tops', 'accessories'): 19, ('tops', 'sunglasses'): 20, ('accessories', 'sunglasses'): 21,\n",
    "    ('accessories', 'bottoms'): 22, ('sunglasses', 'bottoms'): 23, ('all-body', 'all-body'): 24,\n",
    "    ('bags', 'scarves'): 25, ('bags', 'outerwear'): 26, ('shoes', 'scarves'): 27,\n",
    "    ('shoes', 'outerwear'): 28, ('jewellery', 'jewellery'): 29, ('jewellery', 'scarves'): 30,\n",
    "    ('jewellery', 'outerwear'): 31, ('all-body', 'scarves'): 32, ('all-body', 'outerwear'): 33,\n",
    "    ('scarves', 'outerwear'): 34, ('bags', 'accessories'): 35, ('accessories', 'all-body'): 36,\n",
    "    ('bags', 'hats'): 37, ('shoes', 'hats'): 38, ('jewellery', 'hats'): 39, ('hats', 'all-body'): 40,\n",
    "    ('hats', 'outerwear'): 41, ('tops', 'outerwear'): 42, ('bottoms', 'outerwear'): 43,\n",
    "    ('tops', 'scarves'): 44, ('accessories', 'scarves'): 45, ('accessories', 'outerwear'): 46,\n",
    "    ('bottoms', 'scarves'): 47, ('tops', 'all-body'): 48, ('hats', 'tops'): 49,\n",
    "    ('hats', 'sunglasses'): 50, ('hats', 'bottoms'): 51, ('sunglasses', 'outerwear'): 52,\n",
    "    ('bags', 'bags'): 53, ('shoes', 'shoes'): 54, ('hats', 'accessories'): 55,\n",
    "    ('scarves', 'scarves'): 56, ('tops', 'tops'): 57, ('all-body', 'bottoms'): 58,\n",
    "    ('sunglasses', 'scarves'): 59, ('hats', 'scarves'): 60, ('accessories', 'accessories'): 61,\n",
    "    ('bottoms', 'bottoms'): 62, ('outerwear', 'outerwear'): 63, ('hats', 'hats'): 64,  ('sunglasses', 'sunglasses'): 65}\n",
    "\n",
    "def get_unique_items(dictionary):\n",
    "    unique_items = []\n",
    "\n",
    "    for pair in dictionary:\n",
    "        item1, item2 = pair\n",
    "        if item1 not in unique_items: unique_items.append(item1)\n",
    "        if item2 not in unique_items: unique_items.append(item2)\n",
    "\n",
    "    return unique_items\n",
    "\n",
    "unique_items = list(get_unique_items(data))\n",
    "#print(len(unique_items), unique_items)\n",
    "\n",
    "pairs_items = list([p for p in itertools.product(unique_items, repeat=2)])\n",
    "#print(len(pairs_items), pairs_items)\n",
    "\n",
    "typespaces_csa = dict(zip(pairs_items, range(len(pairs_items))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a01c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "typespaces_csa\n",
    "\n",
    "print(typespaces_csa[('jewellery', 'shoes')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca99fa6",
   "metadata": {},
   "source": [
    "## 2. Instalación de los modelos de aprendizaje\n",
    "### 2.1 Instalación del modelo BYOL\n",
    "\n",
    "   Se instala la variante del modelo BYOL del trabajo de *Javier Morales Rodriguez* del repositorio *bimodal-byol-shoes*:\n",
    "- https://github.com/javier-op/bimodal-byol-shoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3b5b248",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b6cc19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a9e6721",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf5bc925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile('runs/new_model_type_aware/checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fb37a73",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint 'runs/precise_models/csa_adapt_2_tloss_5_conditions_20_epochs/model_best.pth.tar'\n",
      "=> loaded checkpoint 'runs/precise_models/csa_adapt_2_tloss_5_conditions_20_epochs/model_best.pth.tar' (epoch 3)\n",
      "\n",
      "test set: Compat AUC: 0.86 FITB: 54.4\n",
      " FITB_var: 61.9\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raraya/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3450: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import Resnet_18\n",
    "from polyvore_outfits_csa import TripletImageLoader\n",
    "from tripletnet import Tripletnet\n",
    "from tripletnet_original import Tripletnet as tripletnet_org\n",
    "from ConditionalSimNet_adapt import ConditionalSimNet, ConditionalSimNet2, ConditionalSimNetNew\n",
    "import sys\n",
    "\n",
    "\n",
    "test_arg = True\n",
    "use_fc_arg = False\n",
    "learned_arg = True\n",
    "prein_arg = True\n",
    "epochs_args = 20\n",
    "batch_size_args = 96\n",
    "learning_rate_args = 5e-5\n",
    "num_conditions = 5\n",
    "weight_decay_arg = 1e-4\n",
    "margin_arg = 0.3\n",
    "lr_scheduler_arg = 0.9\n",
    "\n",
    "# names of model\n",
    "name_arg = 'csa_adapt_2_tloss_%s_conditions_%s_epochs' % (num_conditions, epochs_args)\n",
    "resume_arg =  'runs/precise_models/%s/model_best.pth.tar' % (name_arg)\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='Fashion Compatibility Example')\n",
    "parser.add_argument('--batch-size', type=int, default=batch_size_args, metavar='N',\n",
    "                    help='input batch size for training (default: 256)')\n",
    "parser.add_argument('--epochs', type=int, default=epochs_args, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--start_epoch', type=int, default=1, metavar='N',\n",
    "                    help='number of start epoch (default: 1)')\n",
    "parser.add_argument('--lr', type=float, default=learning_rate_args, metavar='LR',\n",
    "                    help='learning rate (default: 5e-5)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--resume', default=resume_arg, type=str,\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "\n",
    "parser.add_argument('--name', default=name_arg, type=str,\n",
    "                    help='name of experiment')\n",
    "parser.add_argument('--polyvore_split', default='nondisjoint', type=str,\n",
    "                    help='specifies the split of the polyvore data (either disjoint or nondisjoint)')\n",
    "parser.add_argument('--datadir', default='../../polyvore_type_aware_data', type=str,\n",
    "                    help='directory of the polyvore outfits dataset (default: data)')\n",
    "parser.add_argument('--test', dest='test', action='store_true', default=test_arg,\n",
    "                    help='To only run inference on test set')\n",
    "parser.add_argument('--dim_embed', type=int, default=64, metavar='N',\n",
    "                    help='how many dimensions in embedding (default: 64)')\n",
    "parser.add_argument('--use_fc', action='store_true', default=use_fc_arg,\n",
    "                    help='Use a fully connected layer to learn type specific embeddings.')\n",
    "parser.add_argument('--learned', dest='learned', action='store_true', default=learned_arg,\n",
    "                    help='To learn masks from random initialization')\n",
    "parser.add_argument('--prein', dest='prein', action='store_true', default=prein_arg,\n",
    "                    help='To initialize masks to be disjoint')\n",
    "parser.add_argument('--num_heads', dest='num_heads', action='store_true', default=4,\n",
    "                    help='Amount of heads on')\n",
    "\n",
    "parser.add_argument('--rand_typespaces', action='store_true', default=False,\n",
    "                    help='randomly assigns comparisons to type-specific embeddings where #comparisons < #embeddings')\n",
    "parser.add_argument('--num_rand_embed', type=int, default=4, metavar='N',\n",
    "                    help='number of random embeddings when rand_typespaces=True')\n",
    "parser.add_argument('--l2_embed', dest='l2_embed', action='store_true', default=False,\n",
    "                    help='L2 normalize the output of the type specific embeddings')\n",
    "parser.add_argument('--learned_metric', dest='learned_metric', action='store_true', default=False,\n",
    "                    help='Learn a distance metric rather than euclidean distance')\n",
    "parser.add_argument('--margin', type=float, default=margin_arg, metavar='M',\n",
    "                    help='margin for triplet loss (default: 0.2)')\n",
    "parser.add_argument('--embed_loss', type=float, default=5e-4, metavar='M',\n",
    "                    help='parameter for loss for embedding norm')\n",
    "parser.add_argument('--mask_loss', type=float, default=5e-4, metavar='M',\n",
    "                    help='parameter for loss for mask norm')\n",
    "parser.add_argument('--vse_loss', type=float, default=5e-3, metavar='M',\n",
    "                    help='parameter for loss for the visual-semantic embedding')\n",
    "parser.add_argument('--sim_t_loss', type=float, default=5e-5, metavar='M',\n",
    "                    help='parameter for loss for text-text similarity')\n",
    "parser.add_argument('--sim_i_loss', type=float, default=5e-5, metavar='M',\n",
    "                    help='parameter for loss for image-image similarity')\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def main():\n",
    "    global args\n",
    "    # torch.set_printoptions(threshold=10_000)\n",
    "    torch.set_printoptions(profile=\"full\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    \n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    fn = os.path.join(args.datadir, 'polyvore_outfits', 'polyvore_item_metadata.json')\n",
    "    meta_data = json.load(open(fn, 'r'))\n",
    "    text_feature_dim = 6000\n",
    "    kwargs = {'num_workers': 8, 'pin_memory': True} if args.cuda else {}\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        TripletImageLoader(args, 'test', meta_data, typespaces_csa,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(112),\n",
    "                               transforms.CenterCrop(112),\n",
    "                               transforms.ToTensor(),\n",
    "                               normalize,\n",
    "                           ])),\n",
    "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "    \n",
    "    typespaces = typespaces_csa\n",
    "    \n",
    "    # definition of the model\n",
    "    ## Type aware model\n",
    "    model = Resnet_18.resnet18(pretrained=True, embedding_size=args.dim_embed)\n",
    "    csn_model = ConditionalSimNet2(args, model, num_conditions, typespaces)\n",
    "\n",
    "    criterion = torch.nn.MarginRankingLoss(margin = args.margin)\n",
    "    tnet = Tripletnet(args, csn_model, text_feature_dim, criterion)\n",
    "    if args.cuda:\n",
    "        tnet.cuda()\n",
    "    \n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        TripletImageLoader(args, 'train', meta_data, typespaces_csa,\n",
    "                           text_dim=text_feature_dim,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(112),\n",
    "                               transforms.CenterCrop(112),\n",
    "                               transforms.RandomHorizontalFlip(),\n",
    "                               transforms.ToTensor(),\n",
    "                               normalize,\n",
    "                           ])),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    \n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        TripletImageLoader(args, 'valid', meta_data, typespaces_csa,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(112),\n",
    "                               transforms.CenterCrop(112),\n",
    "                               transforms.ToTensor(),\n",
    "                               normalize,\n",
    "                           ])),\n",
    "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    best_acc = 0\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume,encoding='latin1')\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_acc = checkpoint['best_prec1']\n",
    "            tnet.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                    .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "    cudnn.benchmark = True    \n",
    "    if args.test:\n",
    "        test_acc = test(test_loader, tnet)\n",
    "        sys.exit()\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, tnet.parameters())\n",
    "    optimizer = optim.Adam(parameters, lr=args.lr, betas=(0.9, 0.999))\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=lr_scheduler_arg)\n",
    "    \n",
    "    n_parameters = sum([p.data.nelement() for p in tnet.parameters()])\n",
    "    print('  + Number of params: {}'.format(n_parameters))\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs + 1):\n",
    "        # update learning rate\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        # train for one epoch\n",
    "        train(train_loader, tnet, criterion, optimizer, epoch)\n",
    "        \n",
    "        # evaluate on validation set\n",
    "        acc = test(val_loader, tnet)\n",
    "\n",
    "        # remember best acc and save checkpoint\n",
    "        is_best = acc > best_acc\n",
    "        best_acc = max(acc, best_acc)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': tnet.state_dict(),\n",
    "            'best_prec1': best_acc,\n",
    "        }, is_best)\n",
    "        \n",
    "        #lr_scheduler.step()\n",
    "        \n",
    "\n",
    "    checkpoint = torch.load('runs/precise_models/%s/'%(args.name) + 'model_best.pth.tar')\n",
    "    tnet.load_state_dict(checkpoint['state_dict'])\n",
    "    test_acc = test(test_loader, tnet)\n",
    "\n",
    "def train_original(train_loader, tnet, criterion, optimizer, epoch):\n",
    "    losses = AverageMeter()\n",
    "    accs = AverageMeter()\n",
    "    emb_norms = AverageMeter()\n",
    "    mask_norms = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    tnet.train()\n",
    "    for batch_idx, (img1, desc1, has_text1, img2, desc2, has_text2, img3, desc3, has_text3, condition) in enumerate(train_loader):\n",
    "        \n",
    "        anchor = TrainData(img1, desc1, has_text1, condition)\n",
    "        close = TrainData(img2, desc2, has_text2)\n",
    "        far = TrainData(img3, desc3, has_text3)\n",
    "\n",
    "        # compute output\n",
    "        acc, loss_triplet, loss_mask, loss_embed, loss_vse, loss_sim_t, loss_sim_i = tnet(anchor, far, close)\n",
    "        \n",
    "        # encorages similar text inputs (sim_t) and image inputs (sim_i) to\n",
    "        # embed close to each other, images operate on the general embedding\n",
    "        loss_sim = args.sim_t_loss * loss_sim_t + args.sim_i_loss * loss_sim_i\n",
    "        \n",
    "        # cross-modal similarity regularizer on the general embedding\n",
    "        loss_vse_w = args.vse_loss * loss_vse\n",
    "        \n",
    "        # sparsity and l2 regularizer\n",
    "        loss_reg = args.embed_loss * loss_embed + args.mask_loss * loss_mask\n",
    "\n",
    "        loss = loss_triplet + loss_reg\n",
    "        if args.vse_loss > 0:\n",
    "            loss += loss_vse_w\n",
    "        if args.sim_t_loss > 0 or args.sim_i_loss > 0:\n",
    "            loss += loss_sim\n",
    "            \n",
    "        num_items = len(anchor)\n",
    "        # measure accuracy and record loss\n",
    "        \n",
    "        losses.update(loss_triplet.item(), num_items)\n",
    "        accs.update(acc.item(), num_items)\n",
    "        emb_norms.update(loss_embed.item())\n",
    "        #mask_norms.update(loss_mask.item())\n",
    "            \n",
    "        # compute gradient and do optimizer step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if loss == loss:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{}]\\t'\n",
    "                  'Loss: {:.4f} ({:.4f}) \\t'\n",
    "                  'Acc: {:.2f}% ({:.2f}%) \\t'\n",
    "                  'Emb_Norm: {:.2f} ({:.2f})'.format(\n",
    "                epoch, batch_idx * num_items, len(train_loader.dataset),\n",
    "                losses.val, losses.avg, \n",
    "                100. * accs.val, 100. * accs.avg, emb_norms.val, emb_norms.avg))\n",
    "            \n",
    "\n",
    "def train(train_loader, tnet, criterion, optimizer, epoch):\n",
    "    losses = AverageMeter()\n",
    "    accs = AverageMeter()\n",
    "    emb_norms = AverageMeter()\n",
    "    mask_norms = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    tnet.train()\n",
    "    for batch_idx, (img1, desc1, has_text1, img2, desc2, has_text2, img3, desc3, has_text3, condition) in enumerate(train_loader):\n",
    "        \n",
    "        anchor = TrainData(img1, desc1, has_text1, condition)\n",
    "        close = TrainData(img2, desc2, has_text2)\n",
    "        far = TrainData(img3, desc3, has_text3)\n",
    "\n",
    "        # compute output\n",
    "        acc, loss_triplet, loss_mask, loss_embed, loss_sim_i = tnet(anchor, far, close)\n",
    "        \n",
    "        # encorages similar text inputs (sim_t) and image inputs (sim_i) to\n",
    "        # embed close to each other, images operate on the general embedding\n",
    "        #loss_sim = args.sim_i_loss * loss_sim_i\n",
    "        \n",
    "        # cross-modal similarity regularizer on the general embedding\n",
    "        #loss_vse_w = args.vse_loss * loss_vse\n",
    "        \n",
    "        # sparsity and l2 regularizer\n",
    "        #loss_reg = args.embed_loss * loss_embed + args.mask_loss * loss_mask\n",
    "\n",
    "        loss = loss_triplet #+ loss_reg + loss_sim\n",
    "        #if args.vse_loss > 0:\n",
    "        #    loss += loss_vse_w\n",
    "        #if args.sim_t_loss > 0 or args.sim_i_loss > 0:\n",
    "        #    loss += loss_sim\n",
    "            \n",
    "        num_items = len(anchor)\n",
    "        # measure accuracy and record loss\n",
    "        \n",
    "        losses.update(loss_triplet.item(), num_items)\n",
    "        accs.update(acc.item(), num_items)\n",
    "        emb_norms.update(loss_embed.item())\n",
    "        #mask_norms.update(loss_mask.item())\n",
    "        \n",
    "        # compute gradient and do optimizer step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if loss == loss:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{}]\\t'\n",
    "                  'Loss: {:.4f} ({:.4f}) \\t'\n",
    "                  'Acc: {:.2f}% ({:.2f}%) \\t'\n",
    "                  'Emb_Norm: {:.2f} ({:.2f})'.format(\n",
    "                epoch, batch_idx * num_items, len(train_loader.dataset),\n",
    "                losses.val, losses.avg, \n",
    "                100. * accs.val, 100. * accs.avg, emb_norms.val, emb_norms.avg))\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"Saves checkpoint to disk\"\"\"\n",
    "    directory = \"runs/precise_models/%s/\"%(args.name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filename = directory + filename\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'runs/precise_models/%s/'%(args.name) + 'model_best.pth.tar')\n",
    "        \n",
    "        \n",
    "def test(test_loader, tnet):\n",
    "    # switch to evaluation mode\n",
    "    tnet.eval()\n",
    "    embeddings = []\n",
    "    \n",
    "    # for test/val data we get images only from the data loader\n",
    "    for batch_idx, images in enumerate(test_loader):\n",
    "        if args.cuda:\n",
    "            images = images.cuda()\n",
    "        images = Variable(images)\n",
    "        embeddings.append(tnet.embeddingnet(images).data)\n",
    "        \n",
    "    embeddings = torch.cat(embeddings)\n",
    "    metric = tnet.metric_branch\n",
    "    auc = test_loader.dataset.test_compatibility(embeddings, metric)\n",
    "    acc = test_loader.dataset.test_fitb(embeddings, metric)\n",
    "    acc_var = test_loader.dataset.test_fitb_var(embeddings, metric)\n",
    "    total = auc + acc + acc_var\n",
    "    print('\\n{} set: Compat AUC: {:.2f} FITB: {:.1f}\\n FITB_var: {:.1f}\\n'.format(\n",
    "        test_loader.dataset.split,\n",
    "        round(auc, 2), round(acc * 100, 1), round(acc_var * 100, 1)))\n",
    "    \n",
    "    return total\n",
    "\n",
    "\n",
    "class TrainData():\n",
    "    def __init__(self, images, text, has_text, conditions = None):\n",
    "        has_text = has_text.float()\n",
    "        if args.cuda:\n",
    "            images, text, has_text = images.cuda(), text.cuda(), has_text.cuda()\n",
    "        images, text, has_text = Variable(images), Variable(text), Variable(has_text)\n",
    "        \n",
    "        if conditions is not None and not args.use_fc:\n",
    "            if args.cuda:\n",
    "                conditions = conditions.cuda()\n",
    "\n",
    "            conditions = Variable(conditions)\n",
    "        \n",
    "        self.images = images\n",
    "        self.text = text\n",
    "        self.has_text = has_text\n",
    "        self.conditions = conditions\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.images.size(0)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.lr * ((1 - 0.2) ** epoch)\n",
    "    print('learning rate: ', lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.argv = ['--test', '--l2_embed']\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6fe928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "images = None\n",
    "images_embed_0 = None\n",
    "images_embed_1 = None\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba58c3c",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento y resultados en las métricas de evaluación\n",
    "\n",
    "Se entrenan los modelos definidos en la sección anterior, usando el *DataLoader* para cargar los datos. \n",
    "\n",
    "**IMPORTANTE**: *SelectFromTuple* no funciona bien. Por mientras se modifica BYOL2_model pero revisar bien la transformacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(os.path.dirname(os.getcwd()), 'plot_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0bf4f",
   "metadata": {},
   "source": [
    "## 4. Ejemplo de sistema de recuperacion\n",
    "\n",
    "Del modelo entrenado se obtienen los embeddings de las imagenes de testeo y se obtienen las prendas que mas combinan con una imagen en particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71e0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# si ocurre error DefaultCPUAllocator: can't allocate memory, reducir el num_workers y batch_size\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import time\n",
    "import Resnet_18\n",
    "from polyvore_outfits import TripletImageLoader\n",
    "from tripletnet import Tripletnet\n",
    "from ConditionalSimNet_adapt import ConditionalSimNet\n",
    "import sys\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "\n",
    "test_arg = False\n",
    "use_fc_arg = False\n",
    "learned_arg = True\n",
    "prein_arg = True\n",
    "epochs_args = 5\n",
    "batch_size_args = 256\n",
    "learning_rate_args = 5e-5\n",
    "num_conditions = 5\n",
    "\n",
    "# names of model\n",
    "name_arg = 'csa_adapt_%s_conditions_%s_epochs' % (num_conditions, epochs_args)\n",
    "resume_arg =  'runs/precise_models/%s/model_best.pth.tar' % (name_arg)\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='Fashion Compatibility Example')\n",
    "parser.add_argument('--batch-size', type=int, default=batch_size_args, metavar='N',\n",
    "                    help='input batch size for training (default: 256)')\n",
    "parser.add_argument('--epochs', type=int, default=epochs_args, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--start_epoch', type=int, default=1, metavar='N',\n",
    "                    help='number of start epoch (default: 1)')\n",
    "parser.add_argument('--lr', type=float, default=learning_rate_args, metavar='LR',\n",
    "                    help='learning rate (default: 5e-5)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--resume', default=resume_arg, type=str,\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "\n",
    "parser.add_argument('--name', default=name_arg, type=str,\n",
    "                    help='name of experiment')\n",
    "parser.add_argument('--polyvore_split', default='nondisjoint', type=str,\n",
    "                    help='specifies the split of the polyvore data (either disjoint or nondisjoint)')\n",
    "parser.add_argument('--datadir', default='../../polyvore_type_aware_data', type=str,\n",
    "                    help='directory of the polyvore outfits dataset (default: data)')\n",
    "parser.add_argument('--test', dest='test', action='store_true', default=test_arg,\n",
    "                    help='To only run inference on test set')\n",
    "parser.add_argument('--dim_embed', type=int, default=64, metavar='N',\n",
    "                    help='how many dimensions in embedding (default: 64)')\n",
    "parser.add_argument('--use_fc', action='store_true', default=use_fc_arg,\n",
    "                    help='Use a fully connected layer to learn type specific embeddings.')\n",
    "parser.add_argument('--learned', dest='learned', action='store_true', default=learned_arg,\n",
    "                    help='To learn masks from random initialization')\n",
    "parser.add_argument('--prein', dest='prein', action='store_true', default=prein_arg,\n",
    "                    help='To initialize masks to be disjoint')\n",
    "parser.add_argument('--num_heads', dest='num_heads', action='store_true', default=4,\n",
    "                    help='Amount of heads on')\n",
    "\n",
    "parser.add_argument('--rand_typespaces', action='store_true', default=False,\n",
    "                    help='randomly assigns comparisons to type-specific embeddings where #comparisons < #embeddings')\n",
    "parser.add_argument('--num_rand_embed', type=int, default=4, metavar='N',\n",
    "                    help='number of random embeddings when rand_typespaces=True')\n",
    "parser.add_argument('--l2_embed', dest='l2_embed', action='store_true', default=True,\n",
    "                    help='L2 normalize the output of the type specific embeddings')\n",
    "parser.add_argument('--learned_metric', dest='learned_metric', action='store_true', default=False,\n",
    "                    help='Learn a distance metric rather than euclidean distance')\n",
    "parser.add_argument('--margin', type=float, default=0.3, metavar='M',\n",
    "                    help='margin for triplet loss (default: 0.2)')\n",
    "parser.add_argument('--embed_loss', type=float, default=5e-4, metavar='M',\n",
    "                    help='parameter for loss for embedding norm')\n",
    "parser.add_argument('--mask_loss', type=float, default=5e-4, metavar='M',\n",
    "                    help='parameter for loss for mask norm')\n",
    "parser.add_argument('--vse_loss', type=float, default=5e-3, metavar='M',\n",
    "                    help='parameter for loss for the visual-semantic embedding')\n",
    "parser.add_argument('--sim_t_loss', type=float, default=5e-5, metavar='M',\n",
    "                    help='parameter for loss for text-text similarity')\n",
    "parser.add_argument('--sim_i_loss', type=float, default=5e-5, metavar='M',\n",
    "                    help='parameter for loss for image-image similarity')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def choose_random_elements(arr1, arr2, index, n):\n",
    "    matching_indices = [i for i in range(len(arr2)) if arr2[i] == index]\n",
    "    return random.sample(matching_indices, n)\n",
    "\n",
    "def default_image_loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "def main(): \n",
    "    global args\n",
    "    # torch.set_printoptions(threshold=10_000)\n",
    "    torch.set_printoptions(profile=\"full\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    \n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    fn = os.path.join(args.datadir, 'polyvore_outfits', 'polyvore_item_metadata.json')\n",
    "    meta_data = json.load(open(fn, 'r'))\n",
    "    text_feature_dim = 6000\n",
    "    kwargs = {'num_workers': 8, 'pin_memory': True} if args.cuda else {}\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        TripletImageLoader(args, 'test', meta_data,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(112),\n",
    "                               transforms.CenterCrop(112),\n",
    "                               transforms.ToTensor(),\n",
    "                               normalize,\n",
    "                           ]), return_image_path=True),\n",
    "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    \n",
    "    typespaces = test_loader.dataset.typespaces\n",
    "    \n",
    "    #print(typespaces)\n",
    "    \n",
    "    \n",
    "    # definition of the model\n",
    "    ## Type aware model\n",
    "    model = Resnet_18.resnet18(pretrained=True, embedding_size=args.dim_embed)\n",
    "    csn_model = ConditionalSimNet(args, model, num_conditions, typespaces)\n",
    "\n",
    "    criterion = torch.nn.MarginRankingLoss(margin = args.margin)\n",
    "    tnet = Tripletnet(args, csn_model, text_feature_dim, criterion)\n",
    "    if args.cuda:\n",
    "        tnet.cuda()\n",
    "    \n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume,encoding='latin1')\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_acc = checkpoint['best_prec1']\n",
    "            tnet.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                    .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "    \n",
    "    \n",
    "    # guardar la lista de embeddings de las imagenes, la lista de las categorias y la lista del path de las imagenes\n",
    "    # para retirarlas\n",
    "    embeddings_list, cat_list, images_list = [], [], []\n",
    "    \n",
    "    # for test/val data we get images only from the data loader\n",
    "    for batch_idx, images_info in enumerate(test_loader):\n",
    "        images, images_path, imgcat = images_info\n",
    "    \n",
    "        if args.cuda:\n",
    "            images = images.cuda()\n",
    "        images = Variable(images)\n",
    "        \n",
    "        # se guarda el embedding de la imagen, el path de la imagen y la categoria\n",
    "        img_embedding = tnet.embeddingnet(images).data\n",
    "        embeddings_list.append(img_embedding)\n",
    "        images_list.append(images_path)\n",
    "        cat_list.append(imgcat)\n",
    "    \n",
    "    # embeddings de las imagenes, path de las imagenes y categorias en formato de tensor de torch\n",
    "    embeddings_list = torch.cat(embeddings_list)\n",
    "    images_list = list(itertools.chain.from_iterable(images_list))\n",
    "    cat_list = list(itertools.chain.from_iterable(cat_list))\n",
    "    \n",
    "    # se selecciona una condicion de typespaces de forma aleatoria\n",
    "    typespaces = test_loader.dataset.typespaces\n",
    "    example_cond = [9]\n",
    "    # random.sample(range(0, 66), 1)\n",
    "    \n",
    "    # se obtienen las embeddings que solamente corresponden a la condicion establecida por example_cond\n",
    "    # y se retiran las categorias asociadas a esa clase\n",
    "    embeddings_list = embeddings_list[:, example_cond, :].squeeze().cpu()\n",
    "    condition_value = {i for i in typespaces if typespaces[i]==example_cond[0]}\n",
    "    class1, class2 = condition_value.pop()\n",
    "    \n",
    "    # leave only the vectors or images that correspond to class 1 or class 2\n",
    "    embeddings_list_class1, cat_list_class1, images_list_class1 = [], [], []\n",
    "    embeddings_list_class2, cat_list_class2, images_list_class2 = [], [], []\n",
    "    \n",
    "    for i in range(len(embeddings_list)):\n",
    "        if cat_list[i] == class1:\n",
    "            img_embedding = torch.unsqueeze(embeddings_list[i,:].clone().detach(), 0)\n",
    "            embeddings_list_class1.append(img_embedding)\n",
    "            cat_list_class1.append(cat_list[i])\n",
    "            images_list_class1.append(images_list[i])\n",
    "        \n",
    "        if cat_list[i] == class2:\n",
    "            img_embedding = torch.unsqueeze(embeddings_list[i,:].clone().detach(), 0)\n",
    "            embeddings_list_class2.append(img_embedding)\n",
    "            cat_list_class2.append(cat_list[i])\n",
    "            images_list_class2.append(images_list[i])\n",
    "    \n",
    "    #embeddings_list_class1 = torch.cat(embeddings_list_class1)\n",
    "    \n",
    "    # se eliguen n ejemplos que correspondan a la clase 1\n",
    "    n_ejemplos = 5\n",
    "    ejemplos = range(n_ejemplos)\n",
    "    \n",
    "    num_tensors = len(embeddings_list_class1)\n",
    "    random_indices = random.choices(range(num_tensors), k=n_ejemplos)\n",
    "    \n",
    "    ejemplos_class_1 = [embeddings_list_class1[i] for i in random_indices]\n",
    "    ejemplos_class_1_path = [images_list_class1[i] for i in random_indices]\n",
    "    \n",
    "    ejemplos_class_1.extend(embeddings_list_class2)\n",
    "    ejemplos_class_1 = torch.cat(ejemplos_class_1)\n",
    "    \n",
    "    ejemplos_class_1_path.extend(images_list_class2)\n",
    "    \n",
    "    # se define el modelo NearestNeighbors\n",
    "    n_nbrs = 10\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_nbrs).fit(ejemplos_class_1)\n",
    "    near_nei = nbrs.kneighbors(ejemplos_class_1[ejemplos, :], return_distance=False)\n",
    "    \n",
    "    # crear figura y subplots\n",
    "    fig, ax = plt.subplots(n_ejemplos, n_nbrs, figsize=(20,15))\n",
    "    \n",
    "    for j in range(n_ejemplos):\n",
    "        for i in range(n_nbrs):\n",
    "            image_id = near_nei[j, i]\n",
    "            print(ejemplos_class_1_path[image_id])\n",
    "            example_img = default_image_loader(ejemplos_class_1_path[image_id])\n",
    "            ax[j,i].imshow(example_img, interpolation='nearest')\n",
    "            titulo = str(image_id)\n",
    "            \n",
    "            ax[j, i].axis('off')\n",
    "        print()\n",
    "\n",
    "    \n",
    "    # titulo de la figura\n",
    "    fig.suptitle('Resultados: Conjuntos de imagenes compatibles para las clases %s, %s' % (class1, class2), fontsize=24)\n",
    "    \n",
    "    # PSETS OF COMPATIBLE CLOTHES FOLDER\n",
    "    image_sets_dir = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'plot_results', 'image_sets', args.name)\n",
    "    \n",
    "    if not os.path.exists(image_sets_dir): os.makedirs(image_sets_dir)\n",
    "    plt.savefig(os.path.join(image_sets_dir, '{}.png'.format(int(time.time()))))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    sys.argv = ['']\n",
    "    main()\n",
    "    \n",
    "    \n",
    "# torch.Size([256, 67, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f459c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results = os.path.join(os.path.dirname(os.getcwd()), 'plot_results')\n",
    "print(plot_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718d6ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[28273, 20866, 29961],[12343, 45634, 29961], [0, 0, 0], [130, 1600, 781], [1500, 1700, 30]])\n",
    "knn = NearestNeighbors(n_neighbors=4)\n",
    "knn.fit(X)\n",
    "\n",
    "knn.kneighbors(X[0].reshape(1,-1), return_distance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe49bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostrar los datos en X, Y\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "brs = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "print(indices)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "x = X[:,0]\n",
    "y = X[:,1]\n",
    "\n",
    "ax.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf34785",
   "metadata": {},
   "source": [
    "## 4. Visualización de imágenes y *augmentaciones*\n",
    "\n",
    "A continuación, como paso intermedio, se visualizan las imágenes y las augmentaciones para alimentar al modelo. Esto se importante puesto que la eficiencia depende del modelo depende directamente de las augmentaciones que se eligan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e89ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# librerías importantes e importar BYOL\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from polyvore_dataset_loader import DoubletImageLoader\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "from bimodal_byol_shoes.data.custom_transforms import BatchTransform, ListToTensor, PadToSquare, SelectFromTuple, TensorToDevice\n",
    "from bimodal_byol_shoes.models.BYOL2_model import BYOL2\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "    \n",
    "def main():\n",
    "\n",
    "\n",
    "     # path to important folders\n",
    "    # path to important folders\n",
    "    polyvore_dataset = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'fashion_proj', 'polyvore_data')\n",
    "    polyvore_images = os.path.join(polyvore_dataset, 'images')\n",
    "    polyvore_info = os.path.join(polyvore_dataset, 'polyvore-info')\n",
    "    category_info = os.path.join(polyvore_info, 'category_id.txt')\n",
    "    \n",
    "    d = {}\n",
    "    with open(category_info) as f:\n",
    "        for line in f:\n",
    "            (key, val) = line.split(' ', 1)\n",
    "            d[int(key)] = val\n",
    "            \n",
    "    category_data = open(category_info, 'r')\n",
    "    \n",
    "     # asegurarse que la carpeta exista\n",
    "    models_folder = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'fashion_proj', 'fashion_models', 'checkpoint_models')\n",
    "\n",
    "    # revisa si hay gpu cuda sino ocupa cpu\n",
    "    no_cuda = False # cambiar si se quiere ocupar  cuda\n",
    "    cuda = not no_cuda and torch.cuda.is_available()   # CAMBIAR SI SE POSEEN RECURSOS COMO GPU\n",
    "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "    log_interval = 10\n",
    "    text_feature_dim = 6000\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "    # pre-cargar modelos y evaluar en dataset de validacion o testeo\n",
    "    val_arg = False\n",
    "    test_arg = True\n",
    "    resume = os.path.join(models_folder, 'model_best.pth.tar')\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    transforms_1 = transforms.Compose([SelectFromTuple(0), TensorToDevice(device)])\n",
    "    transforms_2 = transforms.Compose([SelectFromTuple(1), TensorToDevice(device)])\n",
    "\n",
    "    # otros modelos importantes, codificador, se inicializa el modelo\n",
    "    encoder = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "    #encoder.load_state_dict(torch.load('../checkpoints/resnet50_byol_quickdraw_128_1000_v3.pt'))\n",
    "    empty_transform = transforms.Compose([])\n",
    "    epochs = 1\n",
    "    epoch_size = 300\n",
    "    byol_learner = BYOL2(\n",
    "        encoder,\n",
    "        image_size=224,\n",
    "        hidden_layer='avgpool',\n",
    "        augment_fn=empty_transform,\n",
    "        cosine_ema_steps=epochs*epoch_size\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    byol_learner.augment1 = transforms_1\n",
    "    byol_learner.augment2 = transforms_2\n",
    "\n",
    "    # data parallel\n",
    "    if cuda:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        byol_learner = nn.DistributedDataParallel(byol_learner)\n",
    "    byol_learner.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    if resume:\n",
    "        if os.path.isfile(resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "            checkpoint = torch.load(resume)\n",
    "\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            best_val_loss = checkpoint['best_prec1']\n",
    "            byol_learner.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                    .format(resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "\n",
    "\n",
    "    # augmentación to apply\n",
    "    augmentation = transforms.Compose([\n",
    "                           transforms.Resize((224, 224)),\n",
    "                           transforms.RandomHorizontalFlip(),\n",
    "                           normalize])\n",
    "\n",
    "\n",
    "\n",
    "    # otros modelos importantes, codificador, se inicializa el modelo\n",
    "    encoder = models.resnet50(weights='DEFAULT')\n",
    "\n",
    "    # dataloader for the validation data\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        DoubletImageLoader('test', polyvore_images, polyvore_info, transform=transforms.ToTensor()),\n",
    "        batch_size=1, shuffle=True)\n",
    "    \n",
    "    \n",
    "    ## COMENTAR DESPUES\n",
    "    # dataloaders for training and validation da\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        DoubletImageLoader('train', polyvore_images, polyvore_info,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize((224, 224)),\n",
    "                               transforms.RandomHorizontalFlip(),\n",
    "                               transforms.ToTensor(),\n",
    "                               normalize,\n",
    "                           ]), return_image_path=True),\n",
    "        batch_size=1, shuffle=True, **kwargs)\n",
    "    \n",
    "    # get random image from the batch\n",
    "    for batch_idx, image_info in enumerate(train_loader):\n",
    "        img1, img1category, anchor_im, img2, img2category, pos_im, img3, img3category, neg_im = image_info\n",
    "    ## HASTA ACA\n",
    "\n",
    "    # get random image from the batch\n",
    "    sample_tensor = None\n",
    "    sample_category = None\n",
    "    for batch_idx, image_info in enumerate(val_loader):\n",
    "        sample_tensor, sample_category = image_info\n",
    "        break\n",
    "\n",
    "    # preprocess the tensor and augment it\n",
    "    sample_tensor = sample_tensor.squeeze()\n",
    "    augmentated_tensor = augmentation(sample_tensor)\n",
    "\n",
    "    # use the created array to output your multiple images. In this case I have stacked 4 images vertically\n",
    "    sample_image = np.transpose(sample_tensor, (1, 2, 0))\n",
    "    augmentated_image = np.transpose(augmentated_tensor, (1, 2, 0))\n",
    "    \n",
    "    image_cat = d[int(sample_category[0])]\n",
    "    \n",
    "    \n",
    "    # crear figura y subplots\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    ax[0].imshow(sample_image, interpolation='nearest')\n",
    "    ax[0].set_title('Imagen original', fontweight =\"bold\")\n",
    "    ax[0].axis('off')\n",
    "\n",
    "\n",
    "    ax[1].imshow(augmentated_image, interpolation='nearest')\n",
    "    ax[1].set_title('Imagen Augmentada', fontweight =\"bold\")\n",
    "    ax[1].axis('off')\n",
    "\n",
    "    # titulo de la figura\n",
    "    fig.suptitle('Muestra de augmentación de imágen. Categoria {}'.format(image_cat), fontsize=16)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c898b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b41251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
