{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4651c468",
   "metadata": {},
   "source": [
    "# Entrenamiento de modelos de aprendizaje con el dataset Polyvore\n",
    "\n",
    "**Autor:** Roberto Araya Day\n",
    "\n",
    "**Fecha de inicio:** 16/11/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da9e3d4",
   "metadata": {},
   "source": [
    "## 1. Preparación de datos\n",
    "\n",
    "- El archivo *type_aware_polyvore_outfits.py* es el script de **Learning Type Aware Embeddings for Fashion Compatibility** para cargar el dataset de Polyvore y crear ejemplos. Se define un **TripletImageLoader** con una serie de transformaciones para las imágenes. Este archivo retorna *Triplets* de imágenes postivas y negativas. Modificamos este archivo en la forma de **DoubletImageLoader** para retornar pares de imágenes. Revisar el archivo. \n",
    "- El archivo *similarity_conditions_triplet_image_loader.py* es el script de **Learning similarity Conditions** para cargar triplets de datos de Polyvore. Revisar.\n",
    "\n",
    "- Se dividen en los archivos:\n",
    "    - ***fashion_compatibility_prediction.txt***: En ***fashion_compatibility_prediction_val.txt*** y ***fashion_compatibility_prediction_test.txt*** para las métricas de validación y testeo. Puesto que el archivo contiene **7076** columnas, se ponen **3538** ejemplos en cada archivo.\n",
    "    - ***fill_in_blank_test.txt***.\n",
    "---\n",
    "\n",
    "Los modelos de recomendación están conformados por la siguiente estructura:\n",
    "1. **DoubletImageLoader**: Carga las imágenes, con sus metadatos, y se crean los ejemplos de pares de prendas positivos. Es decir, prendas compatibles. Esta basado en los dataloader de ambas investigaciones señaladas anteriormente.\n",
    "    - Un DataLoader es una estructura que combina un dataset y un sampler, y provee un iterador sobre el dataset.\n",
    "\n",
    "\n",
    "2. **DoubletNet**: Genera los embeddings y calcula la pérdida y distancia entre los vectores obtenidos de los ejemplos positivos. No se necesita para este caso, puesto que el **Modelo General** lo hace por si solo. No necesita obtener el loss o accuracy de los pares de imágenes.\n",
    "\n",
    "\n",
    "3. **Modelo General**: Variante del modelo BYOL que recibe dos imágenes de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bb267e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/raraya/fashion_proj/fashion_proj/polyvore_data/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m contador \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      8\u001b[0m polyvore_dataset \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(os\u001b[38;5;241m.\u001b[39mgetcwd())), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfashion_proj\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolyvore_data\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m outfit_folder_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolyvore_dataset\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     10\u001b[0m     outfit_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(polyvore_dataset, outfit_folder_name)\n\u001b[1;32m     11\u001b[0m     outfit_clothes \u001b[38;5;241m=\u001b[39m [ os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(outfit_folder, s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(outfit_folder)]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/raraya/fashion_proj/fashion_proj/polyvore_data/images'"
     ]
    }
   ],
   "source": [
    "# path de las imágenes de polyvore\n",
    "import os\n",
    "import itertools\n",
    "\n",
    "# carpeta de las imágenes del dataset\n",
    "# obtiene todas las imágenes en las carpetas (donde están guardados los modelos)\n",
    "contador = 0\n",
    "polyvore_dataset = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'fashion_proj', 'polyvore_data', 'images')\n",
    "for outfit_folder_name in os.listdir(polyvore_dataset):\n",
    "    outfit_folder = os.path.join(polyvore_dataset, outfit_folder_name)\n",
    "    outfit_clothes = [ os.path.join(outfit_folder, s) for s in os.listdir(outfit_folder)]\n",
    "    pairs = list(itertools.combinations(outfit_clothes, 2))\n",
    "    contador +=1\n",
    "    if contador == 20: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "455af9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/raraya/miniconda3/envs/fashion_model/lib/python3.10/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce651484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to important folders\n",
    "polyvore_dataset = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'fashion_proj', 'polyvore_data')\n",
    "polyvore_images = os.path.join(polyvore_dataset, 'images')\n",
    "polyvore_info = os.path.join(polyvore_dataset, 'polyvore-info')\n",
    "\n",
    "polyvore_train = os.path.join(polyvore_info, 'train_no_dup')\n",
    "polyvore_val = os.path.join(polyvore_info, 'valid_no_dup')\n",
    "polyvore_test = os.path.join(polyvore_info, 'test_no_dup')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca99fa6",
   "metadata": {},
   "source": [
    "## 2. Instalación de los modelos de aprendizaje\n",
    "### 2.1 Instalación del modelo BYOL\n",
    "\n",
    "   Se instala la variante del modelo BYOL del trabajo de *Javier Morales Rodriguez* del repositorio *bimodal-byol-shoes*:\n",
    "- https://github.com/javier-op/bimodal-byol-shoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e3b5b248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b6cc19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a9e6721",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1fb37a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  + Number of params: 3191808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raraya/fashion_proj/fashion_models/type-aware_model/type_specific_network_attention_3.py:77: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attention_weights = torch.nn.functional.softmax(attention_weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([256, 64])\n",
      "torch.Size([256, 64])\n",
      "torch.Size([256, 64, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "batch1 must be a 3D tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 339\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    338\u001b[0m     sys\u001b[38;5;241m.\u001b[39margv \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--test\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--l2_embed\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 339\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 182\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    180\u001b[0m adjust_learning_rate(optimizer, epoch)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# train for one epoch\u001b[39;00m\n\u001b[0;32m--> 182\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# evaluate on validation set\u001b[39;00m\n\u001b[1;32m    184\u001b[0m acc \u001b[38;5;241m=\u001b[39m test(val_loader, tnet)\n",
      "Cell \u001b[0;32mIn[51], line 214\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, tnet, criterion, optimizer, epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m far \u001b[38;5;241m=\u001b[39m TrainData(img3, desc3, has_text3)\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# compute output\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m acc, loss_triplet, loss_mask, loss_embed, loss_sim_i \u001b[38;5;241m=\u001b[39m \u001b[43mtnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43manchor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# encorages similar text inputs (sim_t) and image inputs (sim_i) to\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# embed close to each other, images operate on the general embedding\u001b[39;00m\n\u001b[1;32m    218\u001b[0m loss_sim \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39msim_i_loss \u001b[38;5;241m*\u001b[39m loss_sim_i\n",
      "File \u001b[0;32m~/miniconda3/envs/fashion_model/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/fashion_proj/fashion_models/type-aware_model/tripletnet.py:152\u001b[0m, in \u001b[0;36mTripletnet.forward\u001b[0;34m(self, x, y, z)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, z):\n\u001b[1;32m    148\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" x: Anchor data\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;03m        y: Distant (negative) data\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;03m        z: Close (positive) data\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     acc, loss_triplet, loss_sim_i, loss_mask, loss_embed, general_x, general_y, general_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m#loss_sim_t, desc_x, desc_y, desc_z = self.text_forward(x, y, z)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m#loss_vse_x = self.calc_vse_loss(desc_x, general_x, general_y, general_z, x.has_text)\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m#loss_vse_y = self.calc_vse_loss(desc_y, general_y, general_x, general_z, y.has_text)\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m#loss_vse_z = self.calc_vse_loss(desc_z, general_z, general_x, general_y, z.has_text)\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m#loss_vse = (loss_vse_x + loss_vse_y + loss_vse_z) / 3.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;66;03m#return acc, loss_triplet, loss_mask, loss_embed, loss_vse, loss_sim_t, loss_sim_i\u001b[39;00m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m acc, loss_triplet, loss_mask, loss_embed, loss_sim_i\n",
      "File \u001b[0;32m~/fashion_proj/fashion_models/type-aware_model/tripletnet.py:75\u001b[0m, in \u001b[0;36mTripletnet.image_forward\u001b[0;34m(self, x, y, z)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# conditions only available on the anchor sample\u001b[39;00m\n\u001b[1;32m     74\u001b[0m c \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mconditions\n\u001b[0;32m---> 75\u001b[0m embedded_x, masknorm_norm_x, embed_norm_x, general_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddingnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m embedded_y, masknorm_norm_y, embed_norm_y, general_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddingnet(y\u001b[38;5;241m.\u001b[39mimages, c)\n\u001b[1;32m     77\u001b[0m embedded_z, masknorm_norm_z, embed_norm_z, general_z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddingnet(z\u001b[38;5;241m.\u001b[39mimages, c)\n",
      "File \u001b[0;32m~/miniconda3/envs/fashion_model/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/fashion_proj/fashion_models/type-aware_model/type_specific_network_attention_3.py:83\u001b[0m, in \u001b[0;36mTypeSpecificNetAttention.forward\u001b[0;34m(self, x, c)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(embedded_x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(embedded_x\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 83\u001b[0m masked_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedded_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2_norm:\n\u001b[1;32m     86\u001b[0m     norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(masked_embedding, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-10\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: batch1 must be a 3D tensor"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import Resnet_18\n",
    "from polyvore_outfits import TripletImageLoader\n",
    "from tripletnet import Tripletnet\n",
    "from type_specific_network import TypeSpecificNet\n",
    "from type_specific_network_attention_3 import TypeSpecificNetAttention\n",
    "import sys\n",
    "\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser(description='Fashion Compatibility Example')\n",
    "parser.add_argument('--batch-size', type=int, default=256, metavar='N',\n",
    "                    help='input batch size for training (default: 256)')\n",
    "parser.add_argument('--epochs', type=int, default=5, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--start_epoch', type=int, default=1, metavar='N',\n",
    "                    help='number of start epoch (default: 1)')\n",
    "parser.add_argument('--lr', type=float, default=5e-5, metavar='LR',\n",
    "                    help='learning rate (default: 5e-5)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--resume', default='', type=str,\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "\n",
    "parser.add_argument('--name', default='attention_model', type=str,\n",
    "                    help='name of experiment')\n",
    "parser.add_argument('--polyvore_split', default='nondisjoint', type=str,\n",
    "                    help='specifies the split of the polyvore data (either disjoint or nondisjoint)')\n",
    "parser.add_argument('--datadir', default='../../polyvore_type_aware_data', type=str,\n",
    "                    help='directory of the polyvore outfits dataset (default: data)')\n",
    "parser.add_argument('--test', dest='test', action='store_true', default=False,\n",
    "                    help='To only run inference on test set')\n",
    "parser.add_argument('--dim_embed', type=int, default=64, metavar='N',\n",
    "                    help='how many dimensions in embedding (default: 64)')\n",
    "parser.add_argument('--use_fc', action='store_true', default=False,\n",
    "                    help='Use a fully connected layer to learn type specific embeddings.')\n",
    "parser.add_argument('--learned', dest='learned', action='store_true', default=True,\n",
    "                    help='To learn masks from random initialization')\n",
    "parser.add_argument('--prein', dest='prein', action='store_true', default=True,\n",
    "                    help='To initialize masks to be disjoint')\n",
    "parser.add_argument('--rand_typespaces', action='store_true', default=False,\n",
    "                    help='randomly assigns comparisons to type-specific embeddings where #comparisons < #embeddings')\n",
    "parser.add_argument('--num_rand_embed', type=int, default=4, metavar='N',\n",
    "                    help='number of random embeddings when rand_typespaces=True')\n",
    "parser.add_argument('--l2_embed', dest='l2_embed', action='store_true', default=True,\n",
    "                    help='L2 normalize the output of the type specific embeddings')\n",
    "parser.add_argument('--learned_metric', dest='learned_metric', action='store_true', default=False,\n",
    "                    help='Learn a distance metric rather than euclidean distance')\n",
    "parser.add_argument('--margin', type=float, default=0.3, metavar='M',\n",
    "                    help='margin for triplet loss (default: 0.2)')\n",
    "parser.add_argument('--embed_loss', type=float, default=5e-4, metavar='M',\n",
    "                    help='parameter for loss for embedding norm')\n",
    "parser.add_argument('--mask_loss', type=float, default=5e-4, metavar='M',\n",
    "                    help='parameter for loss for mask norm')\n",
    "parser.add_argument('--vse_loss', type=float, default=5e-3, metavar='M',\n",
    "                    help='parameter for loss for the visual-semantic embedding')\n",
    "parser.add_argument('--sim_t_loss', type=float, default=5e-5, metavar='M',\n",
    "                    help='parameter for loss for text-text similarity')\n",
    "parser.add_argument('--sim_i_loss', type=float, default=5e-5, metavar='M',\n",
    "                    help='parameter for loss for image-image similarity')\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def main():\n",
    "    global args\n",
    "    # torch.set_printoptions(threshold=10_000)\n",
    "    torch.set_printoptions(profile=\"full\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    \n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    fn = os.path.join(args.datadir, 'polyvore_outfits', 'polyvore_item_metadata.json')\n",
    "    meta_data = json.load(open(fn, 'r'))\n",
    "    text_feature_dim = 6000\n",
    "    kwargs = {'num_workers': 8, 'pin_memory': True} if args.cuda else {}\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        TripletImageLoader(args, 'test', meta_data,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(112),\n",
    "                               transforms.CenterCrop(112),\n",
    "                               transforms.ToTensor(),\n",
    "                               normalize,\n",
    "                           ])),\n",
    "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "    \n",
    "    # definition of the model\n",
    "    ## Type aware model\n",
    "    model = Resnet_18.resnet18(pretrained=True, embedding_size=args.dim_embed)\n",
    "    csn_model = TypeSpecificNetAttention(args, model, len(test_loader.dataset.typespaces))\n",
    "\n",
    "    criterion = torch.nn.MarginRankingLoss(margin = args.margin)\n",
    "    tnet = Tripletnet(args, csn_model, text_feature_dim, criterion)\n",
    "    if args.cuda:\n",
    "        tnet.cuda()\n",
    "        \n",
    "        \n",
    "    ## CSA-Net model\n",
    "    #csn_model = ConditionalSimNet(self.config)\n",
    "    #tnet = Tripletnet(args, csn_model, text_feature_dim, criterion)\n",
    "    #if args.cuda:\n",
    "    #    tnet.cuda()\n",
    "        \n",
    "    # termination of definition of the model\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        TripletImageLoader(args, 'train', meta_data,\n",
    "                           text_dim=text_feature_dim,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(112),\n",
    "                               transforms.CenterCrop(112),\n",
    "                               transforms.RandomHorizontalFlip(),\n",
    "                               transforms.ToTensor(),\n",
    "                               normalize,\n",
    "                           ])),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        TripletImageLoader(args, 'valid', meta_data,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(112),\n",
    "                               transforms.CenterCrop(112),\n",
    "                               transforms.ToTensor(),\n",
    "                               normalize,\n",
    "                           ])),\n",
    "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    best_acc = 0\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume,encoding='latin1')\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_acc = checkpoint['best_prec1']\n",
    "            tnet.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                    .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "\n",
    "    cudnn.benchmark = True    \n",
    "    if args.test:\n",
    "        test_acc = test(test_loader, tnet)\n",
    "        sys.exit()\n",
    "\n",
    "    parameters = filter(lambda p: p.requires_grad, tnet.parameters())\n",
    "    optimizer = optim.Adam(parameters, lr=args.lr)\n",
    "    n_parameters = sum([p.data.nelement() for p in tnet.parameters()])\n",
    "    print('  + Number of params: {}'.format(n_parameters))\n",
    "\n",
    "    for epoch in range(args.start_epoch, args.epochs + 1):\n",
    "        # update learning rate\n",
    "        adjust_learning_rate(optimizer, epoch)\n",
    "        # train for one epoch\n",
    "        train(train_loader, tnet, criterion, optimizer, epoch)\n",
    "        # evaluate on validation set\n",
    "        acc = test(val_loader, tnet)\n",
    "\n",
    "        # remember best acc and save checkpoint\n",
    "        is_best = acc > best_acc\n",
    "        best_acc = max(acc, best_acc)\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': tnet.state_dict(),\n",
    "            'best_prec1': best_acc,\n",
    "        }, is_best)\n",
    "\n",
    "    checkpoint = torch.load('runs/%s/'%(args.name) + 'model_best.pth.tar')\n",
    "    tnet.load_state_dict(checkpoint['state_dict'])\n",
    "    test_acc = test(test_loader, tnet)\n",
    "\n",
    "def train(train_loader, tnet, criterion, optimizer, epoch):\n",
    "    losses = AverageMeter()\n",
    "    accs = AverageMeter()\n",
    "    emb_norms = AverageMeter()\n",
    "    mask_norms = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    tnet.train()\n",
    "    for batch_idx, (img1, desc1, has_text1, img2, desc2, has_text2, img3, desc3, has_text3, condition) in enumerate(train_loader):\n",
    "    \n",
    "        anchor = TrainData(img1, desc1, has_text1, condition)\n",
    "        close = TrainData(img2, desc2, has_text2)\n",
    "        far = TrainData(img3, desc3, has_text3)\n",
    "\n",
    "        # compute output\n",
    "        acc, loss_triplet, loss_mask, loss_embed, loss_sim_i = tnet(anchor, far, close)\n",
    "        \n",
    "        # encorages similar text inputs (sim_t) and image inputs (sim_i) to\n",
    "        # embed close to each other, images operate on the general embedding\n",
    "        loss_sim = args.sim_i_loss * loss_sim_i\n",
    "        \n",
    "        # cross-modal similarity regularizer on the general embedding\n",
    "        # loss_vse_w = args.vse_loss * loss_vse\n",
    "        \n",
    "        # sparsity and l2 regularizer\n",
    "        loss_reg = args.embed_loss * loss_embed + args.mask_loss * loss_mask\n",
    "\n",
    "        loss = loss_triplet + loss_reg + loss_sim\n",
    "        #if args.vse_loss > 0:\n",
    "        #    loss += loss_vse_w\n",
    "        #if args.sim_t_loss > 0 or args.sim_i_loss > 0:\n",
    "        #    loss += loss_sim\n",
    "            \n",
    "        num_items = len(anchor)\n",
    "        # measure accuracy and record loss\n",
    "        \n",
    "        losses.update(loss_triplet.item(), num_items)\n",
    "        accs.update(acc.item(), num_items)\n",
    "        emb_norms.update(loss_embed.item())\n",
    "        mask_norms.update(loss_mask.item())\n",
    "            \n",
    "        # compute gradient and do optimizer step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if loss == loss:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{}]\\t'\n",
    "                  'Loss: {:.4f} ({:.4f}) \\t'\n",
    "                  'Acc: {:.2f}% ({:.2f}%) \\t'\n",
    "                  'Emb_Norm: {:.2f} ({:.2f})'.format(\n",
    "                epoch, batch_idx * num_items, len(train_loader.dataset),\n",
    "                losses.val, losses.avg, \n",
    "                100. * accs.val, 100. * accs.avg, emb_norms.val, emb_norms.avg))\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    \"\"\"Saves checkpoint to disk\"\"\"\n",
    "    directory = \"runs/%s/\"%(args.name)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filename = directory + filename\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'runs/%s/'%(args.name) + 'model_best.pth.tar')\n",
    "        \n",
    "        \n",
    "def test(test_loader, tnet):\n",
    "    # switch to evaluation mode\n",
    "    tnet.eval()\n",
    "    embeddings = []\n",
    "    \n",
    "    # for test/val data we get images only from the data loader\n",
    "    for batch_idx, images in enumerate(test_loader):\n",
    "        if args.cuda:\n",
    "            images = images.cuda()\n",
    "        images = Variable(images)\n",
    "        embeddings.append(tnet.embeddingnet(images).data)\n",
    "        \n",
    "    embeddings = torch.cat(embeddings)\n",
    "    metric = tnet.metric_branch\n",
    "    auc = test_loader.dataset.test_compatibility(embeddings, metric)\n",
    "    acc = test_loader.dataset.test_fitb(embeddings, metric)\n",
    "    total = auc + acc\n",
    "    print('\\n{} set: Compat AUC: {:.2f} FITB: {:.1f}\\n'.format(\n",
    "        test_loader.dataset.split,\n",
    "        round(auc, 2), round(acc * 100, 1)))\n",
    "    \n",
    "    return total\n",
    "\n",
    "\n",
    "class TrainData():\n",
    "    def __init__(self, images, text, has_text, conditions = None):\n",
    "        has_text = has_text.float()\n",
    "        if args.cuda:\n",
    "            images, text, has_text = images.cuda(), text.cuda(), has_text.cuda()\n",
    "        images, text, has_text = Variable(images), Variable(text), Variable(has_text)\n",
    "        \n",
    "        if conditions is not None and not args.use_fc:\n",
    "            if args.cuda:\n",
    "                conditions = conditions.cuda()\n",
    "\n",
    "            conditions = Variable(conditions)\n",
    "        \n",
    "        self.images = images\n",
    "        self.text = text\n",
    "        self.has_text = has_text\n",
    "        self.conditions = conditions\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.images.size(0)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.lr * ((1 - 0.015) ** epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.argv = ['--test', '--l2_embed']\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf5bc925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile('runs/new_model_type_aware/checkpoint.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c6fe928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "images = None\n",
    "images_embed_0 = None\n",
    "images_embed_1 = None\n",
    "gc.collect()\n",
    "with torch.no_grad():\n",
    "    torch.cuda.empty_cache()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba58c3c",
   "metadata": {},
   "source": [
    "## 3. Entrenamiento y resultados en las métricas de evaluación\n",
    "\n",
    "Se entrenan los modelos definidos en la sección anterior, usando el *DataLoader* para cargar los datos. \n",
    "\n",
    "**IMPORTANTE**: *SelectFromTuple* no funciona bien. Por mientras se modifica BYOL2_model pero revisar bien la transformacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(os.path.dirname(os.getcwd()), 'plot_results')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d0bf4f",
   "metadata": {},
   "source": [
    "## 4. Ejemplo de sistema de recuperacion\n",
    "\n",
    "Del modelo entrenado se obtienen los embeddings de las imagenes de testeo y se obtienen las prendas que mas combinan con una imagen en particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b71e0b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshutil\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# si ocurre error DefaultCPUAllocator: can't allocate memory, reducir el num_workers y batch_size\n",
    "\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import Resnet_18\n",
    "from polyvore_outfits import TripletImageLoader\n",
    "from tripletnet import Tripletnet\n",
    "from type_specific_network import TypeSpecificNet\n",
    "import sys\n",
    "import time\n",
    "from PIL import Image\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Fashion Compatibility Example')\n",
    "parser.add_argument('--batch-size', type=int, default=256, metavar='N',\n",
    "                    help='input batch size for training (default: 256)')\n",
    "parser.add_argument('--epochs', type=int, default=5, metavar='N',\n",
    "                    help='number of epochs to train (default: 10)')\n",
    "parser.add_argument('--start_epoch', type=int, default=1, metavar='N',\n",
    "                    help='number of start epoch (default: 1)')\n",
    "parser.add_argument('--lr', type=float, default=5e-5, metavar='LR',\n",
    "                    help='learning rate (default: 5e-5)')\n",
    "parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                    help='random seed (default: 1)')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='enables CUDA training')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='how many batches to wait before logging training status')\n",
    "parser.add_argument('--resume', default='runs/new_model_type_aware/checkpoint.pth.tar', type=str,\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "parser.add_argument('--name', default='new_model_type_aware', type=str,\n",
    "                    help='name of experiment')\n",
    "parser.add_argument('--polyvore_split', default='nondisjoint', type=str,\n",
    "                    help='specifies the split of the polyvore data (either disjoint or nondisjoint)')\n",
    "parser.add_argument('--datadir', default='../../polyvore_type_aware_data', type=str,\n",
    "                    help='directory of the polyvore outfits dataset (default: data)')\n",
    "parser.add_argument('--test', dest='test', action='store_true', default=False,\n",
    "                    help='To only run inference on test set')\n",
    "parser.add_argument('--dim_embed', type=int, default=64, metavar='N',\n",
    "                    help='how many dimensions in embedding (default: 64)')\n",
    "parser.add_argument('--use_fc', action='store_true', default=False,\n",
    "                    help='Use a fully connected layer to learn type specific embeddings.')\n",
    "parser.add_argument('--learned', dest='learned', action='store_true', default=True,\n",
    "                    help='To learn masks from random initialization')\n",
    "parser.add_argument('--prein', dest='prein', action='store_true', default=False,\n",
    "                    help='To initialize masks to be disjoint')\n",
    "parser.add_argument('--rand_typespaces', action='store_true', default=False,\n",
    "                    help='randomly assigns comparisons to type-specific embeddings where #comparisons < #embeddings')\n",
    "parser.add_argument('--num_rand_embed', type=int, default=4, metavar='N',\n",
    "                    help='number of random embeddings when rand_typespaces=True')\n",
    "parser.add_argument('--l2_embed', dest='l2_embed', action='store_true', default=True,\n",
    "                    help='L2 normalize the output of the type specific embeddings')\n",
    "parser.add_argument('--learned_metric', dest='learned_metric', action='store_true', default=False,\n",
    "                    help='Learn a distance metric rather than euclidean distance')\n",
    "parser.add_argument('--margin', type=float, default=0.3, metavar='M',\n",
    "                    help='margin for triplet loss (default: 0.2)')\n",
    "parser.add_argument('--embed_loss', type=float, default=5e-4, metavar='M',\n",
    "                    help='parameter for loss for embedding norm')\n",
    "parser.add_argument('--mask_loss', type=float, default=5e-4, metavar='M',\n",
    "                    help='parameter for loss for mask norm')\n",
    "parser.add_argument('--vse_loss', type=float, default=5e-3, metavar='M',\n",
    "                    help='parameter for loss for the visual-semantic embedding')\n",
    "parser.add_argument('--sim_t_loss', type=float, default=5e-5, metavar='M',\n",
    "                    help='parameter for loss for text-text similarity')\n",
    "parser.add_argument('--sim_i_loss', type=float, default=5e-5, metavar='M',\n",
    "                    help='parameter for loss for image-image similarity')\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "def choose_random_elements(arr1, arr2, index, n):\n",
    "    matching_indices = [i for i in range(len(arr2)) if arr2[i] == index]\n",
    "    return random.sample(matching_indices, n)\n",
    "\n",
    "def default_image_loader(path):\n",
    "    return Image.open(path).convert('RGB')\n",
    "\n",
    "def main(): \n",
    "    global args\n",
    "    # torch.set_printoptions(threshold=10_000)\n",
    "    torch.set_printoptions(profile=\"full\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    \n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    fn = os.path.join(args.datadir, 'polyvore_outfits', 'polyvore_item_metadata.json')\n",
    "    meta_data = json.load(open(fn, 'r'))\n",
    "    text_feature_dim = 6000\n",
    "    kwargs = {'num_workers': 8, 'pin_memory': True} if args.cuda else {}\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        TripletImageLoader(args, 'test', meta_data,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(112),\n",
    "                               transforms.CenterCrop(112),\n",
    "                               transforms.ToTensor(),\n",
    "                               normalize,\n",
    "                           ]), return_image_path=True),\n",
    "        batch_size=args.batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    model = Resnet_18.resnet18(pretrained=True, embedding_size=args.dim_embed)\n",
    "    csn_model = TypeSpecificNet(args, model, len(test_loader.dataset.typespaces))\n",
    "\n",
    "    criterion = torch.nn.MarginRankingLoss(margin = args.margin)\n",
    "    tnet = Tripletnet(args, csn_model, text_feature_dim, criterion)\n",
    "    if args.cuda:\n",
    "        tnet.cuda()\n",
    "    \n",
    "    if args.resume:\n",
    "        if os.path.isfile(args.resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n",
    "            checkpoint = torch.load(args.resume,encoding='latin1')\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            best_acc = checkpoint['best_prec1']\n",
    "            tnet.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                    .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "    \n",
    "    \n",
    "    # guardar la lista de embeddings de las imagenes, la lista de las categorias y la lista del path de las imagenes\n",
    "    # para retirarlas\n",
    "    embeddings_list, cat_list, images_list = [], [], []\n",
    "    \n",
    "    # for test/val data we get images only from the data loader\n",
    "    for batch_idx, images_info in enumerate(test_loader):\n",
    "        images, images_path, imgcat = images_info\n",
    "    \n",
    "        if args.cuda:\n",
    "            images = images.cuda()\n",
    "        images = Variable(images)\n",
    "        \n",
    "        # se guarda el embedding de la imagen, el path de la imagen y la categoria\n",
    "        img_embedding = tnet.embeddingnet(images).data\n",
    "        embeddings_list.append(img_embedding)\n",
    "        images_list.append(images_path)\n",
    "        cat_list.append(imgcat)\n",
    "    \n",
    "    # embeddings de las imagenes, path de las imagenes y categorias en formato de tensor de torch\n",
    "    embeddings_list = torch.cat(embeddings_list)\n",
    "    images_list = list(itertools.chain.from_iterable(images_list))\n",
    "    cat_list = list(itertools.chain.from_iterable(cat_list))\n",
    "    \n",
    "    # se selecciona una condicion de typespaces de forma aleatoria\n",
    "    typespaces = test_loader.dataset.typespaces\n",
    "    example_cond = [9]\n",
    "    # random.sample(range(0, 66), 1)\n",
    "    \n",
    "    # se obtienen las embeddings que solamente corresponden a la condicion establecida por example_cond\n",
    "    # y se retiran las categorias asociadas a esa clase\n",
    "    embeddings_list = embeddings_list[:, example_cond, :].squeeze().cpu()\n",
    "    condition_value = {i for i in typespaces if typespaces[i]==example_cond[0]}\n",
    "    class1, class2 = condition_value.pop()\n",
    "    \n",
    "    # leave only the vectors or images that correspond to class 1 or class 2\n",
    "    embeddings_list2, cat_list2, images_list2 = [], [], []\n",
    "    for i in range(len(embeddings_list)):\n",
    "        if cat_list[i] == class1 or cat_list[i] == class2:\n",
    "            img_embedding = torch.unsqueeze(embeddings_list[i,:].clone().detach(), 0)\n",
    "            embeddings_list2.append(img_embedding)\n",
    "            cat_list2.append(cat_list[i])\n",
    "            images_list2.append(images_list[i])\n",
    "    \n",
    "    embeddings_list2 = torch.cat(embeddings_list2)\n",
    "    \n",
    "    # se eliguen n ejemplos que correspondan a la clase 1\n",
    "    n_ejemplos = 5\n",
    "    ejemplos = choose_random_elements(embeddings_list2, cat_list2, class1, n_ejemplos)\n",
    "\n",
    "    # se define el modelo NearestNeighbors\n",
    "    n_nbrs = 6\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_nbrs).fit(embeddings_list2)\n",
    "    near_nei = nbrs.kneighbors(embeddings_list2[ejemplos, :], return_distance=False)\n",
    "    \n",
    "    # crear figura y subplots\n",
    "    fig, ax = plt.subplots(n_ejemplos, n_nbrs, figsize=(20,15))\n",
    "    for j in range(n_ejemplos):\n",
    "        for i in range(n_nbrs):\n",
    "            image_id = near_nei[j, i]\n",
    "            example_img = default_image_loader(images_list2[image_id])\n",
    "            ax[j,i].imshow(example_img, interpolation='nearest')\n",
    "            titulo = str(image_id) + ' ' + cat_list2[image_id]\n",
    "            \n",
    "            ax[j, i].set_title(titulo)\n",
    "            ax[j, i].axis('off')\n",
    "\n",
    "    \n",
    "    # titulo de la figura\n",
    "    fig.suptitle('Resultados: Conjuntos de imagenes compatibles para las clases %s, %s' % (class1, class2), fontsize=24)\n",
    "    \n",
    "    # PLOT RESULTS AND SETS OF COMPATIBLE CLOTHES\n",
    "    plot_results = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'plot_results')\n",
    "    current_folder = 'image_sets'\n",
    "    \n",
    "    directory = os.path.join(plot_results, current_folder, args.name)\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    plot_name = os.path.join(directory, '{}.png'.format(int(time.time())))\n",
    "    plt.savefig(plot_name)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    sys.argv = ['']\n",
    "    main()\n",
    "    \n",
    "    \n",
    "# torch.Size([256, 67, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f459c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/raraya/fashion_proj/fashion_models/plot_results\n"
     ]
    }
   ],
   "source": [
    "plot_results = os.path.join(os.path.dirname(os.getcwd()), 'plot_results')\n",
    "print(plot_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "718d6ac1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NearestNeighbors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m28273\u001b[39m, \u001b[38;5;241m20866\u001b[39m, \u001b[38;5;241m29961\u001b[39m],[\u001b[38;5;241m12343\u001b[39m, \u001b[38;5;241m45634\u001b[39m, \u001b[38;5;241m29961\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m130\u001b[39m, \u001b[38;5;241m1600\u001b[39m, \u001b[38;5;241m781\u001b[39m], [\u001b[38;5;241m1500\u001b[39m, \u001b[38;5;241m1700\u001b[39m, \u001b[38;5;241m30\u001b[39m]])\n\u001b[0;32m----> 2\u001b[0m knn \u001b[38;5;241m=\u001b[39m \u001b[43mNearestNeighbors\u001b[49m(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m      3\u001b[0m knn\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m      5\u001b[0m knn\u001b[38;5;241m.\u001b[39mkneighbors(X[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), return_distance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'NearestNeighbors' is not defined"
     ]
    }
   ],
   "source": [
    "X = np.array([[28273, 20866, 29961],[12343, 45634, 29961], [0, 0, 0], [130, 1600, 781], [1500, 1700, 30]])\n",
    "knn = NearestNeighbors(n_neighbors=4)\n",
    "knn.fit(X)\n",
    "\n",
    "knn.kneighbors(X[0].reshape(1,-1), return_distance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbe49bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [1 0]\n",
      " [2 1]\n",
      " [3 4]\n",
      " [4 3]\n",
      " [5 4]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mostrar los datos en X, Y\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "brs = NearestNeighbors(n_neighbors=3, algorithm='ball_tree').fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "print(indices)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "x = X[:,0]\n",
    "y = X[:,1]\n",
    "\n",
    "ax.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf34785",
   "metadata": {},
   "source": [
    "## 4. Visualización de imágenes y *augmentaciones*\n",
    "\n",
    "A continuación, como paso intermedio, se visualizan las imágenes y las augmentaciones para alimentar al modelo. Esto se importante puesto que la eficiencia depende del modelo depende directamente de las augmentaciones que se eligan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e89ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# librerías importantes e importar BYOL\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from polyvore_dataset_loader import DoubletImageLoader\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "from bimodal_byol_shoes.data.custom_transforms import BatchTransform, ListToTensor, PadToSquare, SelectFromTuple, TensorToDevice\n",
    "from bimodal_byol_shoes.models.BYOL2_model import BYOL2\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import itertools\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import time\n",
    "    \n",
    "def main():\n",
    "\n",
    "\n",
    "     # path to important folders\n",
    "    # path to important folders\n",
    "    polyvore_dataset = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'fashion_proj', 'polyvore_data')\n",
    "    polyvore_images = os.path.join(polyvore_dataset, 'images')\n",
    "    polyvore_info = os.path.join(polyvore_dataset, 'polyvore-info')\n",
    "    category_info = os.path.join(polyvore_info, 'category_id.txt')\n",
    "    \n",
    "    d = {}\n",
    "    with open(category_info) as f:\n",
    "        for line in f:\n",
    "            (key, val) = line.split(' ', 1)\n",
    "            d[int(key)] = val\n",
    "            \n",
    "    category_data = open(category_info, 'r')\n",
    "    \n",
    "     # asegurarse que la carpeta exista\n",
    "    models_folder = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), 'fashion_proj', 'fashion_models', 'checkpoint_models')\n",
    "\n",
    "    # revisa si hay gpu cuda sino ocupa cpu\n",
    "    no_cuda = False # cambiar si se quiere ocupar  cuda\n",
    "    cuda = not no_cuda and torch.cuda.is_available()   # CAMBIAR SI SE POSEEN RECURSOS COMO GPU\n",
    "    device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "\n",
    "    log_interval = 10\n",
    "    text_feature_dim = 6000\n",
    "    kwargs = {'num_workers': 4, 'pin_memory': True} if cuda else {}\n",
    "\n",
    "    # pre-cargar modelos y evaluar en dataset de validacion o testeo\n",
    "    val_arg = False\n",
    "    test_arg = True\n",
    "    resume = os.path.join(models_folder, 'model_best.pth.tar')\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    transforms_1 = transforms.Compose([SelectFromTuple(0), TensorToDevice(device)])\n",
    "    transforms_2 = transforms.Compose([SelectFromTuple(1), TensorToDevice(device)])\n",
    "\n",
    "    # otros modelos importantes, codificador, se inicializa el modelo\n",
    "    encoder = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "\n",
    "    #encoder.load_state_dict(torch.load('../checkpoints/resnet50_byol_quickdraw_128_1000_v3.pt'))\n",
    "    empty_transform = transforms.Compose([])\n",
    "    epochs = 1\n",
    "    epoch_size = 300\n",
    "    byol_learner = BYOL2(\n",
    "        encoder,\n",
    "        image_size=224,\n",
    "        hidden_layer='avgpool',\n",
    "        augment_fn=empty_transform,\n",
    "        cosine_ema_steps=epochs*epoch_size\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    byol_learner.augment1 = transforms_1\n",
    "    byol_learner.augment2 = transforms_2\n",
    "\n",
    "    # data parallel\n",
    "    if cuda:\n",
    "        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        byol_learner = nn.DistributedDataParallel(byol_learner)\n",
    "    byol_learner.to(device)\n",
    "\n",
    "\n",
    "\n",
    "    if resume:\n",
    "        if os.path.isfile(resume):\n",
    "            print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "            checkpoint = torch.load(resume)\n",
    "\n",
    "            start_epoch = checkpoint['epoch']\n",
    "            best_val_loss = checkpoint['best_prec1']\n",
    "            byol_learner.load_state_dict(checkpoint['state_dict'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                    .format(resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "\n",
    "\n",
    "    # augmentación to apply\n",
    "    augmentation = transforms.Compose([\n",
    "                           transforms.Resize((224, 224)),\n",
    "                           transforms.RandomHorizontalFlip(),\n",
    "                           normalize])\n",
    "\n",
    "\n",
    "\n",
    "    # otros modelos importantes, codificador, se inicializa el modelo\n",
    "    encoder = models.resnet50(weights='DEFAULT')\n",
    "\n",
    "    # dataloader for the validation data\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        DoubletImageLoader('test', polyvore_images, polyvore_info, transform=transforms.ToTensor()),\n",
    "        batch_size=1, shuffle=True)\n",
    "    \n",
    "    \n",
    "    ## COMENTAR DESPUES\n",
    "    # dataloaders for training and validation da\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        DoubletImageLoader('train', polyvore_images, polyvore_info,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize((224, 224)),\n",
    "                               transforms.RandomHorizontalFlip(),\n",
    "                               transforms.ToTensor(),\n",
    "                               normalize,\n",
    "                           ]), return_image_path=True),\n",
    "        batch_size=1, shuffle=True, **kwargs)\n",
    "    \n",
    "    # get random image from the batch\n",
    "    for batch_idx, image_info in enumerate(train_loader):\n",
    "        img1, img1category, anchor_im, img2, img2category, pos_im, img3, img3category, neg_im = image_info\n",
    "    ## HASTA ACA\n",
    "\n",
    "    # get random image from the batch\n",
    "    sample_tensor = None\n",
    "    sample_category = None\n",
    "    for batch_idx, image_info in enumerate(val_loader):\n",
    "        sample_tensor, sample_category = image_info\n",
    "        break\n",
    "\n",
    "    # preprocess the tensor and augment it\n",
    "    sample_tensor = sample_tensor.squeeze()\n",
    "    augmentated_tensor = augmentation(sample_tensor)\n",
    "\n",
    "    # use the created array to output your multiple images. In this case I have stacked 4 images vertically\n",
    "    sample_image = np.transpose(sample_tensor, (1, 2, 0))\n",
    "    augmentated_image = np.transpose(augmentated_tensor, (1, 2, 0))\n",
    "    \n",
    "    image_cat = d[int(sample_category[0])]\n",
    "    \n",
    "    \n",
    "    # crear figura y subplots\n",
    "    fig, ax = plt.subplots(1,2)\n",
    "    ax[0].imshow(sample_image, interpolation='nearest')\n",
    "    ax[0].set_title('Imagen original', fontweight =\"bold\")\n",
    "    ax[0].axis('off')\n",
    "\n",
    "\n",
    "    ax[1].imshow(augmentated_image, interpolation='nearest')\n",
    "    ax[1].set_title('Imagen Augmentada', fontweight =\"bold\")\n",
    "    ax[1].axis('off')\n",
    "\n",
    "    # titulo de la figura\n",
    "    fig.suptitle('Muestra de augmentación de imágen. Categoria {}'.format(image_cat), fontsize=16)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900c898b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b41251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
